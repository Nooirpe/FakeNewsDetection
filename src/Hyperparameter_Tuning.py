import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, validation_curve, learning_curve
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.pipeline import Pipeline
import joblib
import os
import time
import matplotlib.pyplot as plt
from Utils import load_preprocessed_data, print_data_info, create_model_directory



def get_tuning_params(model_name):
    """L·∫•y parameters ƒë·ªÉ tune cho t·ª´ng model v·ªõi Cross-Validation"""
    if model_name == "Logistic Regression":
        return {
            'tfidf__max_features': [5000, 10000, 20000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'tfidf__max_df': [0.9, 0.95],
            'tfidf__min_df': [1, 2, 5],
            'classifier__C': [0.1, 1, 10, 100],
            'classifier__solver': ['liblinear', 'lbfgs'],
            'classifier__max_iter': [1000, 2000]
        }
    elif model_name == "Random Forest":
        return {
            'tfidf__max_features': [5000, 10000, 20000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'tfidf__max_df': [0.9, 0.95],
            'tfidf__min_df': [1, 2, 5],
            'classifier__n_estimators': [50, 100, 200],
            'classifier__max_depth': [10, 20, None],
            'classifier__min_samples_split': [2, 5, 10]
        }
    elif model_name == "Naive Bayes":
        return {
            'tfidf__max_features': [5000, 10000, 20000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'tfidf__max_df': [0.9, 0.95],
            'tfidf__min_df': [1, 2, 5],
            'classifier__alpha': [0.1, 0.5, 1.0, 2.0, 5.0]
        }
    elif model_name == "Linear SVC":
        return {
            'tfidf__max_features': [5000, 10000, 20000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'tfidf__max_df': [0.9, 0.95],
            'tfidf__min_df': [1, 2, 5],
            'classifier__C': [0.1, 1, 10, 100],
            'classifier__max_iter': [1000, 2000, 5000],
            'classifier__dual': [False]  # Better for sparse features
        }
    else:
        return None

def create_model(model_name):
    """T·∫°o model d·ª±a tr√™n t√™n"""
    if model_name == "Logistic Regression":
        return LogisticRegression(random_state=42)
    elif model_name == "Random Forest":
        return RandomForestClassifier(random_state=42, n_jobs=-1)
    elif model_name == "Naive Bayes":
        return MultinomialNB()
    elif model_name == "Linear SVC":
        return LinearSVC(random_state=42)
    else:
        return None

def create_pipeline(model_name):
    """T·∫°o pipeline v·ªõi TF-IDF + Model"""
    tfidf = TfidfVectorizer(
        stop_words='english',
        max_features=10000,  # Default values
        ngram_range=(1, 2),
        max_df=0.95,
        min_df=2
    )
    
    model = create_model(model_name)
    if model is None:
        return None
    
    pipeline = Pipeline([
        ('tfidf', tfidf),
        ('classifier', model)
    ])
    
    return pipeline

def tune_hyperparameters_cv(X, y, model_name, use_grid_search=False):
    """Tune hyperparameters b·∫±ng Cross-Validation"""
    
    print(f"\n TUNING {model_name.upper()}")
    
    # T·∫°o pipeline
    pipeline = create_pipeline(model_name)
    if pipeline is None:
        print(f"Model {model_name} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£!")
        return None, None, None
    
    # L·∫•y parameter grid
    param_grid = get_tuning_params(model_name)
    if param_grid is None:
        print(f"Kh√¥ng c√≥ parameter grid cho {model_name}!")
        return None, None, None
    
    # T·∫°o StratifiedKFold cho cross-validation
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Ch·ªçn search strategy
    if use_grid_search:
        print("S·ª≠ d·ª•ng GridSearchCV (c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian)...")
        search = GridSearchCV(
            pipeline,
            param_grid,
            cv=cv,
            scoring='f1',
            n_jobs=-1,
            verbose=1
        )
    else:
        print("S·ª≠ d·ª•ng RandomizedSearchCV (nhanh h∆°n)...")
        search = RandomizedSearchCV(
            pipeline,
            param_grid,
            n_iter=30,  # Test 30 combinations
            cv=cv,
            scoring='f1',
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
    
    # B·∫Øt ƒë·∫ßu tuning
    print(f"B·∫Øt ƒë·∫ßu tuning v·ªõi {len(X)} samples...")
    start_time = time.time()
    
    search.fit(X, y)
    
    tune_time = time.time() - start_time
    
    # In k·∫øt qu·∫£
    print(f"\n=== K·∫æT QU·∫¢ TUNING ===")
    print(f"Th·ªùi gian tuning: {tune_time:.2f}s")
    print(f"Best CV F1-score: {search.best_score_:.4f}")
    print(f"Best parameters:")
    for param, value in search.best_params_.items():
        print(f"  {param}: {value}")
    
    return search.best_estimator_, search.best_params_, search.best_score_

def check_overfitting_with_learning_curves(model, X, y, model_name):
    """
    Ki·ªÉm tra overfitting b·∫±ng Learning Curves
    V·∫Ω bi·ªÉu ƒë·ªì train vs validation score theo s·ªë l∆∞·ª£ng samples
    """
    print(f"\n=== KI·ªÇM TRA OVERFITTING - LEARNING CURVES ===")
    
    # T·∫°o range s·ªë samples ƒë·ªÉ test
    train_sizes = np.linspace(0.1, 1.0, 10)
    
    print("ƒêang t√≠nh to√°n learning curves...")
    train_sizes_abs, train_scores, val_scores = learning_curve(
        model, X, y,
        train_sizes=train_sizes,
        cv=5,
        scoring='f1',
        n_jobs=-1,
        random_state=42
    )
    
    # T√≠nh mean v√† std
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    # V·∫Ω bi·ªÉu ƒë·ªì
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Score')
    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    
    plt.plot(train_sizes_abs, val_mean, 'o-', color='red', label='Validation Score')
    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
    
    plt.xlabel('Training Set Size')
    plt.ylabel('F1 Score')
    plt.title(f'Learning Curves - {model_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # Ph√¢n t√≠ch k·∫øt qu·∫£
    final_train_score = train_mean[-1]
    final_val_score = val_mean[-1]
    gap = final_train_score - final_val_score
    
    print(f"Train Score (cu·ªëi): {final_train_score:.4f}")
    print(f"Validation Score (cu·ªëi): {final_val_score:.4f}")
    print(f"Gap (Train - Val): {gap:.4f}")
    
    if gap > 0.1:
        print("C·∫¢NH B√ÅO: C√≥ d·∫•u hi·ªáu OVERFITTING m·∫°nh!")
        print("   ‚Üí Train score cao h∆°n validation score ƒë√°ng k·ªÉ")
    elif gap > 0.05:
        print("C√≥ d·∫•u hi·ªáu overfitting nh·∫π")
        print("   ‚Üí N√™n c√¢n nh·∫Øc regularization")
    else:
        print("Model KH√îNG b·ªã overfitting")
        print("   ‚Üí Train v√† validation scores g·∫ßn nhau")
    
    return {
        'train_score': final_train_score,
        'val_score': final_val_score,
        'gap': gap,
        'overfitting': gap > 0.05
    }



def check_overfitting_comprehensive(model, X, y, model_name):
    """
    Ki·ªÉm tra overfitting to√†n di·ªán
    """
    print(f"\n{'='*60}")
    print(f"üîç PH√ÇN T√çCH OVERFITTING TO√ÄN DI·ªÜN - {model_name}")
    print(f"{'='*60}")
    
    # 1. Learning Curves
    learning_results = check_overfitting_with_learning_curves(model, X, y, model_name)
        
    # 3. Cross-validation v·ªõi train scores
    print(f"\n TRAIN VS VALIDATION SCORES ")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # T√≠nh train scores
    train_scores = []
    val_scores = []
    
    for train_idx, val_idx in cv.split(X, y):
        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
        
        # Fit v√† predict
        model.fit(X_train_fold, y_train_fold)
        
        train_pred = model.predict(X_train_fold)
        val_pred = model.predict(X_val_fold)
        
        train_score = f1_score(y_train_fold, train_pred)
        val_score = f1_score(y_val_fold, val_pred)
        
        train_scores.append(train_score)
        val_scores.append(val_score)
    
    train_mean = np.mean(train_scores)
    val_mean = np.mean(val_scores)
    gap_mean = train_mean - val_mean
    
    print(f"Train F1 (CV): {train_mean:.4f} (+/- {np.std(train_scores)*2:.4f})")
    print(f"Val F1 (CV): {val_mean:.4f} (+/- {np.std(val_scores)*2:.4f})")
    print(f"Gap: {gap_mean:.4f}")
    
    # 4. T·ªïng k·∫øt
    print(f"\nT·ªîNG K·∫æT OVERFITTING ANALYSIS")
   
    
    overall_overfitting = False
    
    if learning_results and learning_results['overfitting']:
        print(" Learning Curves: C√≥ overfitting")
        overall_overfitting = True
    else:
        print(" Learning Curves: Kh√¥ng overfitting")
    
    
    if gap_mean > 0.05:
        print(" Train vs Val: C√≥ overfitting")
        overall_overfitting = True
    else:
        print(" Train vs Val: Kh√¥ng overfitting")
    
    print(f"\n K·∫æT LU·∫¨N CU·ªêI C√ôNG:")
    if overall_overfitting:
        print(" Model B·ªä OVERFITTING!")
        print(" Khuy·∫øn ngh·ªã:")
        print("   - TƒÉng regularization (C nh·ªè h∆°n cho Logistic/SVC)")
        print("   - Gi·∫£m complexity (max_depth nh·ªè h∆°n cho Random Forest)")
        print("   - Th√™m d·ªØ li·ªáu training")
        print("   - Feature selection")
        print("   - Early stopping")
    else:
        print(" Model KH√îNG b·ªã overfitting - T·ªët!")
    
    return {
        'overall_overfitting': overall_overfitting,
        'learning_curves': learning_results,
        'train_val_gap': gap_mean
    }

def evaluate_final_model(best_model, X, y):
    """ƒê√°nh gi√° final model b·∫±ng cross-validation"""
    print("\n=== ƒê√ÅNH GI√Å FINAL MODEL ===")
    
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Evaluate v·ªõi nhi·ªÅu metrics
    scoring = ['accuracy', 'precision', 'recall', 'f1']
    
    print("ƒêang ƒë√°nh gi√° v·ªõi 5-fold cross-validation...")
    results = {}
    
    for metric in scoring:
        scores = cross_val_score(best_model, X, y, cv=cv, scoring=metric)
        results[metric] = {
            'mean': scores.mean(),
            'std': scores.std(),
            'scores': scores
        }
        print(f"{metric.capitalize()}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})")
    
    return results
def evaluate_final_model(best_model, X, y):
    """ƒê√°nh gi√° final model b·∫±ng cross-validation"""
    print("\nƒê√ÅNH GI√Å FINAL MODEL")

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Evaluate v·ªõi nhi·ªÅu metrics
    scoring = ['accuracy', 'precision', 'recall', 'f1']
    
    print("ƒêang ƒë√°nh gi√° v·ªõi 5-fold cross-validation...")
    results = {}
    
    for metric in scoring:
        scores = cross_val_score(best_model, X, y, cv=cv, scoring=metric)
        results[metric] = {
            'mean': scores.mean(),
            'std': scores.std(),
            'scores': scores
        }
        print(f"{metric.capitalize()}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})")
    
    return results

def save_tuned_model(best_model, model_name, best_params, cv_score):
    """L∆∞u model v√† parameters ƒë√£ tune"""
    model_dir = create_model_directory()
    
    # L∆∞u model
    model_path = os.path.join(model_dir, f"Tuned_{model_name.replace(' ', '_')}.pkl")
    joblib.dump(best_model, model_path)
    
    # L∆∞u parameters
    params_path = os.path.join(model_dir, f"Best_Parameters_{model_name.replace(' ', '_')}.txt")
    with open(params_path, 'w', encoding='utf-8') as f:
        f.write(f"Best Model: {model_name}\n")
        f.write(f"Best CV F1-Score: {cv_score:.4f}\n")
        f.write(f"Timestamp: {pd.Timestamp.now()}\n")
        f.write("="*50 + "\n\n")
        f.write("Best Parameters:\n")
        for param, value in best_params.items():
            f.write(f"  {param}: {value}\n")
    
    print(f"\nƒê√£ l∆∞u tuned model: {model_path}")
    print(f"ƒê√£ l∆∞u parameters: {params_path}")
    
    return model_path, params_path

def main():
    print("HYPERPARAMETER TUNING - FAKE NEWS DETECTION (Cross-Validation)")
    
    # Danh s√°ch models c√≥ s·∫µn
    available_models = ["Logistic Regression", "Random Forest", "Naive Bayes", "Linear SVC"]
    
    print("C√°c models c√≥ s·∫µn:")
    for i, model in enumerate(available_models, 1):
        print(f"{i}. {model}")
    
    # Nh·∫≠p t√™n model c·∫ßn tune
    model_choice = input("\nNh·∫≠p s·ªë th·ª© t·ª± ho·∫∑c t√™n model (enter = Naive Bayes): ").strip()
    
    if model_choice.isdigit():
        model_idx = int(model_choice) - 1
        if 0 <= model_idx < len(available_models):
            model_name = available_models[model_idx]
        else:
            print("S·ªë kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng Naive Bayes")
            model_name = "Naive Bayes"
    elif model_choice in available_models:
        model_name = model_choice
    elif not model_choice:
        model_name = "Naive Bayes"
    else:
        print("T√™n model kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng Naive Bayes")
        model_name = "Naive Bayes"
    
    print(f"S·∫Ω tune model: {model_name}")
    
    # Ch·ªçn search strategy
    search_choice = input("\nS·ª≠ d·ª•ng GridSearch? (y/N): ").strip().lower()
    use_grid_search = search_choice in ['y', 'yes']
    
    if use_grid_search:
        print(" GridSearch s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian h∆°n nh∆∞ng t√¨m ƒë∆∞·ª£c params t·ªët h∆°n")
    else:
        print(" RandomizedSearch s·∫Ω nhanh h∆°n v√† v·∫´n cho k·∫øt qu·∫£ t·ªët")
    
    # Ch·ªçn c√≥ ki·ªÉm tra overfitting kh√¥ng
    overfitting_choice = input("\nKi·ªÉm tra Overfitting? (Y/n): ").strip().lower()
    check_overfitting = overfitting_choice not in ['n', 'no']
    
    if check_overfitting:
        print(" S·∫Ω ph√¢n t√≠ch overfitting v·ªõi learning curves v√† validation curves")
    else:
        print(" B·ªè qua ki·ªÉm tra overfitting")
    
    # Load d·ªØ li·ªáu
    print("\n1. Load d·ªØ li·ªáu...")
    X, y = load_preprocessed_data()
    if X is None:
        return
    
    print_data_info(X, y, "Full Dataset")
    
    # Tune hyperparameters
    print("\n2. Tune hyperparameters...")
    best_model, best_params, best_score = tune_hyperparameters_cv(
        X, y, model_name, use_grid_search
    )
    
    if best_model is None:
        print("Tuning th·∫•t b·∫°i!")
        return
    
    # ƒê√°nh gi√° final model
    print("\n3. ƒê√°nh gi√° final model...")
    final_results = evaluate_final_model(best_model, X, y)
    
    # Ki·ªÉm tra overfitting n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    overfitting_results = None
    if check_overfitting:
        print("\n4. Ki·ªÉm tra Overfitting...")
        overfitting_results = check_overfitting_comprehensive(best_model, X, y, model_name)
    
    # L∆∞u model ƒë√£ tune
    step_num = "5" if check_overfitting else "4"
    print(f"\n{step_num}. L∆∞u model ƒë√£ tune...")
    model_path, params_path = save_tuned_model(
        best_model, model_name, best_params, best_score
    )
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    print(f"\nHO√ÄN TH√ÄNH TUNING")
    print(f"Model: {model_name}")
    print(f"Best CV F1-Score: {best_score:.4f}")
    print(f"Final Accuracy: {final_results['accuracy']['mean']:.4f} (+/- {final_results['accuracy']['std']*2:.4f})")
    print(f"Final F1-Score: {final_results['f1']['mean']:.4f} (+/- {final_results['f1']['std']*2:.4f})")
    
    if overfitting_results:
        if overfitting_results['overall_overfitting']:
            print(" Overfitting: C√ì - C·∫ßn xem x√©t l·∫°i parameters!")
        else:
            print(" Overfitting: KH√îNG - Model t·ªët!")
    
    print(f"Model saved: {model_path}")

    return best_model, best_params, final_results, overfitting_results

if __name__ == "__main__":
    main()
